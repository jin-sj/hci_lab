{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE_START = \"baselinestart\"\n",
    "BASELINE_END = \"baselineend\"\n",
    "EASY_START = \"easystart\"\n",
    "EASY_END = \"easyend\"\n",
    "HARD_START = \"hardstart\"\n",
    "HARD_END = \"hardend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Gets the row blocks for easy and hard tasks\n",
    "\"\"\"\n",
    "def read_data(fnirs_path, marker_path):\n",
    "    fnirs_df =  pd.read_csv(fnirs_path, sep='\\t', skiprows=range(4), index_col=False)\n",
    "    marker_df = pd.read_csv(marker_path, sep='\\t', skiprows=range(4), index_col=False)\n",
    "    \n",
    "    merged_df = pd.merge(fnirs_df, marker_df, on=\"Matlab_now\", how=\"left\")\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_row_blocks(merged_df):\n",
    "    easy_start_rows = merged_df.index[merged_df.Stimulus_Label == EASY_START].tolist()\n",
    "    easy_end_rows = merged_df.index[merged_df.Stimulus_Label == EASY_END].tolist()\n",
    "    hard_start_rows = merged_df.index[merged_df.Stimulus_Label == HARD_START].tolist()\n",
    "    hard_end_rows = merged_df.index[merged_df.Stimulus_Label == HARD_END].tolist()\n",
    "    \n",
    "    easy_rows = list(zip(easy_start_rows, easy_end_rows))\n",
    "    hard_rows = list(zip(hard_start_rows, hard_end_rows))\n",
    "    \n",
    "    return (easy_rows, hard_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Return subset of df determined by the indices of the row blocks\n",
    "\"\"\"\n",
    "def get_subsets(merged_df, row_blocks):\n",
    "    tables = []\n",
    "    column_names = [\"Matlab_now\", \"A-DC1\", \"A-DC2\", \"A-DC3\", \"A-DC4\", \"A-DC5\",\n",
    "                    \"A-DC6\", \"A-DC7\", \"A-DC8\", \"B-DC1\", \"B-DC2\", \"B-DC3\", \n",
    "                    \"B-DC4\", \"B-DC5\", \"B-DC6\", \"B-DC7\", \"B-DC8\"]\n",
    "    column_indices = [merged_df.columns.get_loc(c) for c in column_names]\n",
    "    for row_block in row_blocks:\n",
    "        df = merged_df.iloc[row_block[0]:row_block[1], column_indices]\n",
    "        start_time = df.iloc[0][\"Matlab_now\"]\n",
    "        df[\"Matlab_now\"] = df[\"Matlab_now\"] - start_time\n",
    "\n",
    "        tables.append(df)\n",
    "    return tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"runs polyfit on the timeseries data\n",
    "    :param tables: table of blocks of easy / hard tasks\n",
    "    :param difficulty: 0 - easy, 1 - hard; labeling process\n",
    "    \n",
    "    :return: numpy array of\n",
    "    feature_row: \n",
    "        AC-1 gradient, AC-1 intercept, AC-1 mean, AC-2 gradient ... DC-8 gradient DC-8 intercept DC-8 mean difficulty\n",
    "    \n",
    "    \n",
    "    Dictionary of key: channel\n",
    "                           value: (gradient, mean, difficulty)\n",
    "\"\"\"\n",
    "def extract_features(tables, difficulty):\n",
    "    column_names = [\n",
    "                    \"A-DC1_mean\", \"A-DC1_slope\", \"A-DC1_intercept\",\n",
    "                    \"A-DC2_mean\", \"A-DC2_slope\", \"A-DC2_intercept\",\n",
    "                    \"A-DC3_mean\", \"A-DC3_slope\", \"A-DC3_intercept\",\n",
    "                    \"A-DC4_mean\", \"A-DC4_slope\", \"A-DC4_intercept\",\n",
    "                    \"A-DC5_mean\", \"A-DC5_slope\", \"A-DC5_intercept\",\n",
    "                    \"A-DC6_mean\", \"A-DC6_slope\", \"A-DC6_intercept\",\n",
    "                    \"A-DC7_mean\", \"A-DC7_slope\", \"A-DC7_intercept\",\n",
    "                    \"A-DC8_mean\", \"A-DC8_slope\", \"A-DC8_intercept\",\n",
    "                    \"B-DC1_mean\", \"A-DC1_slope\", \"A-DC1_intercept\",\n",
    "                    \"B-DC2_mean\", \"A-DC2_slope\", \"A-DC2_intercept\",\n",
    "                    \"B-DC3_mean\", \"A-DC3_slope\", \"A-DC3_intercept\",\n",
    "                    \"B-DC4_mean\", \"A-DC4_slope\", \"A-DC4_intercept\",\n",
    "                    \"B-DC5_mean\", \"A-DC5_slope\", \"A-DC5_intercept\",\n",
    "                    \"B-DC6_mean\", \"A-DC6_slope\", \"A-DC6_intercept\",\n",
    "                    \"B-DC7_mean\", \"A-DC7_slope\", \"A-DC7_intercept\",\n",
    "                    \"B-DC8_mean\", \"B-DC8_slope\", \"A-DC8_intercept\",\n",
    "                    \"difficulty\"\n",
    "                    ]\n",
    "    df = pd.DataFrame(columns=column_names)\n",
    "    data = []\n",
    "    for table in tables:\n",
    "        x = table[\"Matlab_now\"].values\n",
    "        cols = table.columns[1:]\n",
    "        feature_row_dict = {}\n",
    "        for col in cols[:-1]:\n",
    "            y = table[col].values\n",
    "            slope, intercept = np.poly1d(np.polyfit(x, y, 1))\n",
    "            avg = table[col].mean()\n",
    "            mean_col = col + \"_mean\"\n",
    "            slope_col = col + \"_slope\"\n",
    "            intercept_col = col + \"_intercept\"\n",
    "            feature_row_dict[mean_col] = avg\n",
    "            feature_row_dict[slope_col] = slope\n",
    "            feature_row_dict[intercept_col] = intercept\n",
    "\n",
    "        feature_row_dict[\"difficulty\"] = difficulty\n",
    "        data.append(feature_row_dict)\n",
    "        \n",
    "    return pd.DataFrame(data, columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Extract features from given dataset\n",
    "    :param data_path: Directory containing the files\n",
    "    \n",
    "    :return: gets all the easy and hard features from a given dataset\n",
    "\"\"\"\n",
    "def get_features_for_dataset(data_path):\n",
    "    fnirs_path = os.path.join(os.getcwd(), data_path, \"fNIRSdata.txt\")\n",
    "    marker_path = os.path.join(os.getcwd(), data_path, \"markers.txt\")\n",
    "    merged_df = read_data(fnirs_path, marker_path)\n",
    "    easy_rows, hard_rows = get_row_blocks(merged_df)\n",
    "    \n",
    "    easy_tables = get_subsets(merged_df, easy_rows)\n",
    "    hard_tables = get_subsets(merged_df, hard_rows)\n",
    "    easy_feature_rows = extract_features(easy_tables, 0)\n",
    "    hard_feature_rows = extract_features(hard_tables, 1)\n",
    "    \n",
    "    features = easy_feature_rows.append(hard_feature_rows, ignore_index=True)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_902 = get_features_for_dataset(\"/Users/sjjin/workspace/hci_lab/data/S902/2015-02-26_11-24-48-120\")\n",
    "features_903 = get_features_for_dataset(\"/Users/sjjin/workspace/hci_lab/data/S903/2015-02-27_13-20-42-120\")\n",
    "features_904 = get_features_for_dataset(\"/Users/sjjin/workspace/hci_lab/data/S904/2015-02-27_15-30-27-120\")\n",
    "features_905 = get_features_for_dataset(\"/Users/sjjin/workspace/hci_lab/data/S905/2015-03-02_13-14-35-120\")\n",
    "features_906 = get_features_for_dataset(\"/Users/sjjin/workspace/hci_lab/data/S906/2015-03-05_11-17-38-120\")\n",
    "\n",
    "#train_set = np.vstack([features_902, features_903, features_904, features_905])\n",
    "train_set = features_902.append([features_903, features_904, features_905], ignore_index=True)\n",
    "test_set = features_906"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_set[:, :-1]\n",
    "train_y = train_set[:, -1]\n",
    "test_x = test_set[:, :-1]\n",
    "test_y = test_set[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88, 48)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticRegr = LogisticRegression()\n",
    "logisticRegr.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47619047619047616"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_pred = logisticRegr.predict(test_x)\n",
    "accuracy_score(test_y, log_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8571428571428571"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_y, y_pred) # got 76% before. Seems like there is a 'randomness' to the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-7797fef4a8c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "train_set.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = pd.Series(clf.feature_importances_,index=iris.feature_names).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_threshold = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(clf.feature_importances_ > feature_threshold).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False,  True, False, False,  True, False,  True,\n",
       "        True, False, False,  True, False,  True,  True,  True,  True,\n",
       "       False, False, False,  True, False, False,  True, False, False,\n",
       "       False,  True, False, False, False, False, False, False,  True,\n",
       "        True, False,  True,  True, False,  True,  True, False,  True,\n",
       "        True, False, False])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.feature_importances_ > feature_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01892398, 0.01477054, 0.01209295, 0.03678417, 0.01853434,\n",
       "       0.01479748, 0.02311035, 0.01266382, 0.03106301, 0.02967628,\n",
       "       0.01317049, 0.01849386, 0.02963081, 0.01341022, 0.02524082,\n",
       "       0.02144389, 0.02479585, 0.02576555, 0.01675805, 0.01210709,\n",
       "       0.01158916, 0.04370449, 0.01617765, 0.01943488, 0.02839419,\n",
       "       0.01827404, 0.01873304, 0.01753371, 0.02193131, 0.01505509,\n",
       "       0.0168014 , 0.01369639, 0.01206126, 0.01411264, 0.01598762,\n",
       "       0.02421942, 0.03319256, 0.0184667 , 0.02666304, 0.04395435,\n",
       "       0.01228977, 0.02116911, 0.03467486, 0.01445985, 0.02179242,\n",
       "       0.02461133, 0.01378903, 0.01399715])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
