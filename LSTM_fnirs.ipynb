{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas import Series\n",
    "\n",
    "#from lstm.ivie_data import BiRNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from lstm import ivie_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda_enabled = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE_START = \"baselinestart\"\n",
    "BASELINE_END = \"baselineend\"\n",
    "EASY_START = \"easystart\"\n",
    "EASY_END = \"easyend\"\n",
    "HARD_START = \"hardstart\"\n",
    "HARD_END = \"hardend\"\n",
    "\n",
    "INPUT_SIZE = 352 # determined by the shortest test sample\n",
    "\n",
    "EASY_DIFFICULTY = 0\n",
    "HARD_DIFFICULTY = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Gets the row blocks for easy and hard tasks\n",
    "\"\"\"\n",
    "def read_data(fnirs_path, marker_path):\n",
    "    fnirs_df =  pd.read_csv(fnirs_path, sep='\\t', skiprows=range(4), index_col=False)\n",
    "    marker_df = pd.read_csv(marker_path, sep='\\t', skiprows=range(4), index_col=False)\n",
    "    \n",
    "    merged_df = pd.merge(fnirs_df, marker_df, on=\"Matlab_now\", how=\"left\")\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_row_blocks(merged_df):\n",
    "    easy_start_rows = merged_df.index[merged_df.Stimulus_Label == EASY_START].tolist()\n",
    "    easy_end_rows = merged_df.index[merged_df.Stimulus_Label == EASY_END].tolist()\n",
    "    hard_start_rows = merged_df.index[merged_df.Stimulus_Label == HARD_START].tolist()\n",
    "    hard_end_rows = merged_df.index[merged_df.Stimulus_Label == HARD_END].tolist()\n",
    "    \n",
    "    easy_rows = []\n",
    "    hard_rows = []\n",
    "    if len(easy_start_rows) == len(easy_end_rows):\n",
    "        easy_rows = list(zip(easy_start_rows, easy_end_rows))\n",
    "        for rows in easy_rows:\n",
    "            if rows[0] > rows[1]:\n",
    "                easy_rows = []\n",
    "                print(\"Easy mismatch\")\n",
    "                break\n",
    "    if len(hard_start_rows) == len(hard_end_rows):\n",
    "        hard_rows = list(zip(hard_start_rows, hard_end_rows))\n",
    "        for rows in hard_rows:\n",
    "            if rows[0] > rows[1]:\n",
    "                hard_rows = []\n",
    "                print(\"Hard mismatch\")\n",
    "                break\n",
    "    if len(easy_start_rows) != len(easy_end_rows):\n",
    "        print(\"Easy mismatch\")\n",
    "    if len(hard_start_rows) != len(hard_end_rows):\n",
    "        print(\"Hard mismatch\")\n",
    "    if len(easy_start_rows) != len(easy_end_rows) and len(hard_start_rows) != len(hard_end_rows):\n",
    "        raise ValueError\n",
    " \n",
    "    return (easy_rows, hard_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Return subset of df determined by the indices of the row blocks\n",
    "\"\"\"\n",
    "def get_subsets(merged_df, row_blocks, difficulty):\n",
    "    tables = []\n",
    "    column_names = [\"Matlab_now\", \"A-DC1\", \"A-DC2\", \"A-DC3\", \"A-DC4\", \"A-DC5\",\n",
    "                    \"A-DC6\", \"A-DC7\", \"A-DC8\", \"B-DC1\", \"B-DC2\", \"B-DC3\", \n",
    "                    \"B-DC4\", \"B-DC5\", \"B-DC6\", \"B-DC7\", \"B-DC8\"]\n",
    "    column_indices = [merged_df.columns.get_loc(c) for c in column_names]\n",
    "    for row_block in row_blocks:\n",
    "        df = merged_df.iloc[row_block[0]:row_block[1], column_indices]\n",
    "        start_time = df.iloc[0][\"Matlab_now\"]\n",
    "        df[\"Matlab_now\"] = df[\"Matlab_now\"] - start_time\n",
    "        df[\"Difficulty\"] = difficulty\n",
    "\n",
    "        tables.append(df.iloc[:INPUT_SIZE])\n",
    "    return tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Extract features from given dataset\n",
    "    :param data_path: Directory containing the files\n",
    "    \n",
    "    :return: gets all the easy and hard features from a given dataset\n",
    "\"\"\"\n",
    "def get_data(file_name):\n",
    "    fnirs_path = os.path.join(\"data/clean_data/\") + file_name + \"_fNIRSdata.txt\"\n",
    "    marker_path = os.path.join(\"data/clean_data/\") + file_name + \"_markers.txt\"\n",
    "    merged_df = read_data(fnirs_path, marker_path)\n",
    "    easy_rows, hard_rows = get_row_blocks(merged_df)\n",
    "\n",
    "    easy_tables = get_subsets(merged_df, easy_rows, EASY_DIFFICULTY)\n",
    "    hard_tables = get_subsets(merged_df, hard_rows, HARD_DIFFICULTY)\n",
    "\n",
    "    return easy_tables + hard_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Easy mismatch\n",
      "Easy mismatch\n",
      "Hard mismatch\n",
      "File error S1106\n",
      "Easy mismatch\n",
      "Easy mismatch\n",
      "Hard mismatch\n",
      "Easy mismatch\n",
      "Hard mismatch\n",
      "Hard mismatch\n",
      "Easy mismatch\n",
      "Easy mismatch\n",
      "Hard mismatch\n",
      "File error S1112\n",
      "Easy mismatch\n",
      "Hard mismatch\n",
      "File error S1113\n",
      "Hard mismatch\n",
      "Hard mismatch\n",
      "Easy mismatch\n",
      "Hard mismatch\n",
      "Easy mismatch\n",
      "Easy mismatch\n",
      "Hard mismatch\n",
      "File error S1116\n",
      "Hard mismatch\n",
      "Easy mismatch\n",
      "Hard mismatch\n",
      "Easy mismatch\n",
      "Hard mismatch\n",
      "Hard mismatch\n"
     ]
    }
   ],
   "source": [
    "file_names = [\"S703\", \"S901\", \"S902\", \"S903\", \"S904\", \"S905\",\n",
    "              \"S906\", \"S907\", \"S908\", \"S909\", \"S910\", \"S911\",\n",
    "              \"S912\", \"S913\", \"S1101\", \"S1102\", \"S1103\", \"S1104\",\n",
    "              \"S1105\", \"S1106\", \"S1107\", \"S1108\", \"S1110\",\n",
    "              \"S1111\", \"S1111_2\", \"S1112\", \"S1113\", \"S1114\", \"S1114_2\",\n",
    "              \"S1115\", \"S1116\", \"S1117\", \"S1118\", \"S1119\", \"S1120\"]\n",
    "# bad: \"S1106\", \"S1107\"\n",
    "train = []\n",
    "test = []\n",
    "for fname in file_names[:-6]:\n",
    "    try:\n",
    "        train = train + get_data(fname)\n",
    "    except ValueError:\n",
    "        print(\"File error %s\" % fname)\n",
    "for fname in file_names[-6:]:\n",
    "    try:\n",
    "        test = test + get_data(fname)\n",
    "    except ValueError:\n",
    "        print(\"File error %s\" % fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450 25\n"
     ]
    }
   ],
   "source": [
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"A-DC1\", \"A-DC2\", \"A-DC3\", \"A-DC4\", \"A-DC5\",\n",
    "           \"A-DC6\", \"A-DC7\", \"A-DC8\", \"B-DC1\", \"B-DC2\", \"B-DC3\", \n",
    "           \"B-DC4\", \"B-DC5\", \"B-DC6\", \"B-DC7\", \"B-DC8\", \"Difficulty\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = [a.iloc[:,1:-1] for a in train]\n",
    "train_y = [a.iloc[0,-1] for a in train]\n",
    "test_x = [a.iloc[:,1:-1] for a in test]\n",
    "test_y = [a.iloc[0,-1] for a in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.tensor(targets_df['targets'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([352, 16])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(train_x[0].values).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(train_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(352, 16)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'shuffle': True,\n",
    "          'num_workers': 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fnirs(Dataset):\n",
    "    #Characterizes a dataset for PyTorch\n",
    "    def __init__(self, data, labels):\n",
    "        #Initialization\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        #Denotes the total number of samples\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #Generates one sample of data\n",
    "        x = torch.tensor(self.data[index].values, dtype=torch.float32).to(device)\n",
    "        y = torch.tensor([self.labels[index]], dtype=torch.long).to(device)\n",
    "        return x, y            \n",
    "    \n",
    "    def _normalize(self, df):\n",
    "        normalized_df=(df-df.mean())/df.std()\n",
    "        return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.is_training = False\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_classes = num_classes\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
    "                            batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Dropout(p=0.5, inplace=False)\n",
    "        self.linear = nn.Linear(self.hidden_size*2, self.num_classes)\n",
    "\n",
    "        if cuda_enabled:\n",
    "            self.lstm = self.lstm.cuda()\n",
    "            self.fc = self.fc.cuda()\n",
    "            self.linear = self.linear.cuda()\n",
    "\n",
    "    def forward(self, x): \n",
    "        # Set initial states\n",
    "        h0 = Variable(torch.zeros(self.num_layers*2, x.size(0), self.hidden_size)).to(device) # 2 for bidirection\n",
    "        c0 = Variable(torch.zeros(self.num_layers*2, x.size(0), self.hidden_size)).to(device)\n",
    "        if cuda_enabled:\n",
    "            h0 = h0.cuda()  # 2 for bidirection\n",
    "            c0 = c0.cuda()\n",
    "\n",
    "        # Forward propagate RNN\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        #out, _ = self.lstm(x, h0)\n",
    "        \n",
    "        # Decode hidden state of last time step\n",
    "        if self.is_training:\n",
    "            out = self.fc(out[:, -1, :]) \n",
    "        else:\n",
    "            out = out[:, -1, :]\n",
    "\n",
    "        out = F.log_softmax(self.linear(out), dim=1)\n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32, tensor([[[922.2000, 212.2000,  47.3100,  ...,  44.0000,   9.1740,   3.3400],\n",
      "         [888.2000, 208.0000,  46.1700,  ...,  43.3900,   8.9140,   3.3320],\n",
      "         [892.8000, 204.1000,  45.7100,  ...,  43.6000,   8.8830,   3.3110],\n",
      "         ...,\n",
      "         [925.9000, 215.8000,  47.7100,  ...,  46.2700,   9.6090,   3.5230],\n",
      "         [924.8000, 217.1000,  48.2000,  ...,  45.8900,   9.4670,   3.5100],\n",
      "         [930.3000, 218.2000,  48.9300,  ...,  46.2100,   9.7450,   3.5530]]],\n",
      "       device='cuda:0')\n",
      "torch.int64, tensor([[1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "train_data = fnirs(train_x, train_y)\n",
    "test_data = fnirs(test_x, test_y)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=8, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=8, shuffle=False)\n",
    "for i, (x, y) in enumerate(train_loader):\n",
    "    print(\"%s, %s\" % (x.dtype, x))\n",
    "    print(\"%s, %s\" % (y.dtype, y))\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Current epoch loss: 156.0633\n",
      "Epoch loss: 1000000.0000\n",
      "Epoch 1\n",
      "Current epoch loss: 155.5919\n",
      "Epoch loss: 156.0633\n",
      "Epoch 2\n",
      "Current epoch loss: 156.3251\n",
      "Epoch loss: 155.5919\n",
      "Epoch 3\n",
      "Current epoch loss: 155.2809\n",
      "Epoch loss: 156.3251\n",
      "Epoch 4\n",
      "Current epoch loss: 156.7828\n",
      "Epoch loss: 155.2809\n",
      "Epoch 5\n",
      "Current epoch loss: 156.4348\n",
      "Epoch loss: 156.7828\n",
      "Epoch 6\n",
      "Current epoch loss: 156.0774\n",
      "Epoch loss: 156.4348\n",
      "Epoch 7\n",
      "Current epoch loss: 155.6724\n",
      "Epoch loss: 156.0774\n",
      "Epoch 8\n",
      "Current epoch loss: 156.6689\n",
      "Epoch loss: 155.6724\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "hidden_size = 8\n",
    "num_layers = 5\n",
    "num_classes = 2\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "sequence_length = 16 # column size. get rid of time unless time difference is consistent\n",
    "input_size = 352 # longest length of ti\n",
    "rnn = BiRNN(input_size, hidden_size, num_layers, num_classes)\n",
    "rnn.is_training = True\n",
    "\n",
    "train_data = fnirs(train_x, train_y)\n",
    "test_data = fnirs(test_x, test_y)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "epoch_loss = 1000000.\n",
    "\n",
    "# Train it\n",
    "for epoch in range(num_epochs):\n",
    "    loss_total = 0.\n",
    "    iteration_count = 1.\n",
    "    print(\"Epoch %d\" % epoch)\n",
    "    for i, (data, label) in enumerate(train_loader):\n",
    "        data = Variable(data.view(-1, sequence_length, input_size))\n",
    "\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = rnn(data)\n",
    "\n",
    "        loss = criterion(outputs, label.squeeze_())\n",
    "        loss_total += loss.data\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    current_epoch_loss = loss_total / iteration_count\n",
    "    print(\"Current epoch loss: %.4f\" % current_epoch_loss)\n",
    "    print(\"Epoch loss: %.4f\" % epoch_loss)\n",
    "    # Optimise training epochs: only continue training while the loss drops\n",
    "    if epoch > 5 and current_epoch_loss >= epoch_loss:\n",
    "        break\n",
    "    epoch_loss = current_epoch_loss\n",
    "print(\"DONE\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing -----------------------------------------------\n",
      "12.0\n",
      "25.0\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "# Test the Model\n",
    "rnn.is_training = False\n",
    "correct = 0.0\n",
    "total = 0.0\n",
    "predicted_list = []\n",
    "label_list = []\n",
    "\n",
    "print('Testing -----------------------------------------------')\n",
    "correct = 0.0\n",
    "total = 0.0\n",
    "predicted_list = []\n",
    "label_list = []\n",
    "for data, label in test_loader:\n",
    "    data = Variable(data.view(-1, sequence_length, input_size))\n",
    "    outputs = rnn(data)\n",
    "\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += label.size(0)\n",
    "    for p, l in zip(predicted, label):\n",
    "        predicted_list.append(p)\n",
    "        label_list.append(l)\n",
    "        if p == l:\n",
    "            correct += 1.0\n",
    "\n",
    "        \n",
    "#print(train.get_encoder().classes_)\n",
    "#print(confusion_matrix(label_list, predicted_list))\n",
    "#print('=============================================')\n",
    "#print('Accuracy = %0.4f' % (accuracy_score(label_list, predicted_list)))\n",
    "#print('=============================================')\n",
    "print(correct)\n",
    "print(total)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
